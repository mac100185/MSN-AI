#!/bin/bash
# Script: ai_check_all.sh para distros basadas en Debian, Ubuntu
# Objetivo: Detectar hardware y recomendar modelo de IA local compatible con Ollama.

echo "=== Verificando hardware del sistema ==="
CPU_CORES=$(nproc)
RAM_GB=$(awk '/MemTotal/ {printf "%.0f", $2/1024/1024}' /proc/meminfo)
GPU_INFO=$(lspci | grep -i 'vga\|3d' | grep -i nvidia)
VRAM_GB=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits 2>/dev/null | awk '{sum += $1} END {print int(sum/1024)}')

echo "CPU: $CPU_CORES nรบcleos"
echo "RAM: $RAM_GB GB"
if [ -n "$GPU_INFO" ]; then
  echo "GPU NVIDIA detectada"
  echo "VRAM total: ${VRAM_GB:-0} GB"
else
  echo "No se detectรณ GPU NVIDIA compatible"
fi

echo
echo "=== Anรกlisis y recomendaciรณn ==="

if [ -n "$GPU_INFO" ]; then
  if [ "$VRAM_GB" -ge 80 ]; then
    MODEL="llama3:70b"
    LEVEL="Mรกximo (alto rendimiento, razonamiento complejo)"
  elif [ "$VRAM_GB" -ge 24 ]; then
    MODEL="llama3:8b"
    LEVEL="Avanzado (programaciรณn y tareas generales)"
  else
    MODEL="mistral:7b"
    LEVEL="Eficiente (recomendado para laptops con GPU media)"
  fi
else
  if [ "$RAM_GB" -ge 32 ]; then
    MODEL="mistral:7b"
    LEVEL="Eficiente (modo CPU posible)"
  else
    MODEL="phi3:mini"
    LEVEL="Ligero (para equipos sin GPU y poca RAM)"
  fi
fi

echo "Modelo recomendado: $MODEL"
echo "Nivel de capacidad: $LEVEL"
echo

# Modelos requeridos por defecto
REQUIRED_MODELS=(
  "qwen3-vl:235b-cloud"
  "gpt-oss:120b-cloud"
  "qwen3-coder:480b-cloud"
)

echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
echo "๐ฆ Modelos requeridos por defecto:"
for model in "${REQUIRED_MODELS[@]}"; do
  echo "   - $model"
done
echo "๐ฆ Modelo recomendado adicional: $MODEL"
echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
echo

read -p "ยฟDeseas instalar Ollama y todos los modelos requeridos? (s/n): " CONFIRM
if [[ "$CONFIRM" =~ ^[sS]$ ]]; then
  if ! command -v ollama &>/dev/null; then
    echo "Instalando Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
  fi

  echo ""
  echo "๐ฅ Instalando modelos requeridos..."
  echo "โ๏ธ  NOTA: Este proceso puede tardar bastante tiempo"
  echo ""

  INSTALLED_COUNT=0
  FAILED_COUNT=0

  # Instalar modelos requeridos
  for model in "${REQUIRED_MODELS[@]}"; do
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
    echo "๐ฅ Descargando: $model"
    echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"

    ollama pull "$model"

    if [ $? -eq 0 ]; then
      echo "โ Modelo $model instalado correctamente"
      INSTALLED_COUNT=$((INSTALLED_COUNT + 1))
    else
      echo "โ Error instalando modelo $model"
      FAILED_COUNT=$((FAILED_COUNT + 1))
    fi
    echo ""
  done

  # Instalar modelo recomendado adicional
  echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
  echo "๐ฅ Descargando modelo recomendado: $MODEL"
  echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"

  ollama pull "$MODEL"

  if [ $? -eq 0 ]; then
    echo "โ Modelo $MODEL instalado correctamente"
    INSTALLED_COUNT=$((INSTALLED_COUNT + 1))
  else
    echo "โ Error instalando modelo $MODEL"
    FAILED_COUNT=$((FAILED_COUNT + 1))
  fi

  echo ""
  echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
  echo "๐ Resumen de instalaciรณn:"
  echo "   โ Instalados: $INSTALLED_COUNT"
  echo "   โ Fallidos: $FAILED_COUNT"
  echo "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ"
  echo ""
  echo "Listo. Puedes probar los modelos con: ollama run <modelo>"
else
  echo "Instalaciรณn omitida. Puedes hacerlo manualmente mรกs tarde."
fi
