#!/bin/bash
# AI Setup Script for MSN-AI Docker Container
# Version: 2.1.0
# Author: Alan Mac-Arthur GarcÃ­a DÃ­az
# Email: alan.mac.arthur.garcia.diaz@gmail.com
# License: GNU General Public License v3.0
# Description: Hardware detection and AI model setup for Docker environment

# Don't exit on error - allow script to continue and report issues
set +e

echo "ğŸ¤– MSN-AI Docker - ConfiguraciÃ³n de IA"
echo "======================================"
echo "ğŸ“§ Desarrollado por: Alan Mac-Arthur GarcÃ­a DÃ­az"
echo "âš–ï¸ Licencia: GPL-3.0"
echo "ğŸ³ Modo: Docker Container"
echo "======================================"

# Environment variables
OLLAMA_HOST=${OLLAMA_HOST:-ollama:11434}
OLLAMA_API_KEY=${OLLAMA_API_KEY:-}
SETUP_TIMEOUT=${SETUP_TIMEOUT:-300}  # 5 minutes timeout
MAX_RETRIES=${MAX_RETRIES:-10}       # Maximum connection retries

# Check and display API Key status
check_api_key() {
    if [ -n "$OLLAMA_API_KEY" ]; then
        # Mask the key for display
        MASKED_KEY="${OLLAMA_API_KEY:0:8}***${OLLAMA_API_KEY: -4}"
        echo "ğŸ”‘ API Key detectada: ${MASKED_KEY}"
        return 0
    else
        echo "âš ï¸  API Key no configurada"
        echo "   Los modelos cloud NO funcionarÃ¡n sin OLLAMA_API_KEY"
        return 1
    fi
}

echo ""
check_api_key || echo "   Configure OLLAMA_API_KEY para usar modelos cloud"
echo ""

# Function to wait for Ollama
wait_for_ollama() {
    echo "ğŸ”„ Esperando conexiÃ³n con Ollama..."
    local max_attempts=$MAX_RETRIES
    local attempt=1

    while [ $attempt -le $max_attempts ]; do
        # First check basic connectivity
        if curl -s --max-time 5 "http://${OLLAMA_HOST}/" >/dev/null 2>&1; then
            echo "âœ… ConexiÃ³n bÃ¡sica con Ollama establecida"

            # Then check if API is responding
            if curl -s --max-time 10 "http://${OLLAMA_HOST}/api/tags" >/dev/null 2>&1; then
                echo "âœ… API de Ollama completamente disponible"
                return 0
            else
                echo "â³ Ollama iniciando, API aÃºn no lista..."
            fi
        else
            echo "â³ Esperando que Ollama inicie..."
        fi

        echo "   Intento $attempt/$max_attempts (esperando 10s...)"
        sleep 10
        attempt=$((attempt + 1))
    done

    echo "âŒ No se pudo conectar con Ollama despuÃ©s de $max_attempts intentos"
    echo "   Verifique que el contenedor ollama estÃ© ejecutÃ¡ndose"
    return 1
}

# Function to detect hardware (adapted for container environment)
detect_hardware() {
    echo "ğŸ” Detectando hardware del contenedor..."

    # CPU cores
    CPU_CORES=$(nproc)
    echo "ğŸ–¥ï¸  CPU: $CPU_CORES nÃºcleos"

    # RAM (in container context)
    if [ -f /sys/fs/cgroup/memory/memory.limit_in_bytes ]; then
        # cgroups v1
        RAM_BYTES=$(cat /sys/fs/cgroup/memory/memory.limit_in_bytes)
        RAM_GB=$((RAM_BYTES / 1024 / 1024 / 1024))
    elif [ -f /sys/fs/cgroup/memory.max ]; then
        # cgroups v2
        RAM_BYTES=$(cat /sys/fs/cgroup/memory.max)
        if [ "$RAM_BYTES" = "max" ]; then
            RAM_GB=$(awk '/MemTotal/ {printf "%.0f", $2/1024/1024}' /proc/meminfo)
        else
            RAM_GB=$((RAM_BYTES / 1024 / 1024 / 1024))
        fi
    else
        # Fallback to system memory
        RAM_GB=$(awk '/MemTotal/ {printf "%.0f", $2/1024/1024}' /proc/meminfo)
    fi

    echo "ğŸ’¾ RAM disponible: ${RAM_GB} GB"

    # GPU detection (limited in container)
    GPU_INFO=""
    VRAM_GB=0

    # Check if nvidia-smi is available (NVIDIA Container Toolkit)
    if command -v nvidia-smi >/dev/null 2>&1; then
        GPU_INFO=$(nvidia-smi --query-gpu=name --format=csv,noheader,nounits 2>/dev/null | head -1)
        VRAM_GB=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits 2>/dev/null | head -1 | awk '{print int($1/1024)}')
        echo "ğŸ® GPU: $GPU_INFO"
        echo "ğŸ® VRAM: ${VRAM_GB} GB"
    else
        echo "ğŸ® GPU: No detectada o no disponible en contenedor"
    fi
}

# Function to get default required models
get_required_models() {
    # Cloud models require signin - skip automatic installation
    # Users must signin manually: docker exec -it msn-ai-ollama ollama signin
    # Then pull models: docker exec -it msn-ai-ollama ollama pull qwen3-vl:235b-cloud
    REQUIRED_MODELS=()
}

# Function to recommend model based on hardware
recommend_model() {
    echo ""
    echo "ğŸ§  Analizando configuraciÃ³n Ã³ptima..."

    local MODEL=""
    local LEVEL=""

    if [ -n "$GPU_INFO" ] && [ "$VRAM_GB" -gt 0 ]; then
        # GPU available
        if [ "$VRAM_GB" -ge 80 ]; then
            MODEL="llama3:70b"
            LEVEL="MÃ¡ximo (alto rendimiento, razonamiento complejo)"
        elif [ "$VRAM_GB" -ge 24 ]; then
            MODEL="llama3:8b"
            LEVEL="Avanzado (programaciÃ³n y tareas generales)"
        elif [ "$VRAM_GB" -ge 8 ]; then
            MODEL="mistral:7b"
            LEVEL="Eficiente (equilibrio perfecto)"
        else
            MODEL="phi3:mini"
            LEVEL="Ligero (para GPUs con poca VRAM)"
        fi
    else
        # CPU only
        if [ "$RAM_GB" -ge 32 ]; then
            MODEL="mistral:7b"
            LEVEL="Eficiente (modo CPU, alta RAM)"
        elif [ "$RAM_GB" -ge 16 ]; then
            MODEL="phi3:mini"
            LEVEL="Ligero (modo CPU, RAM media)"
        else
            MODEL="tinyllama"
            LEVEL="BÃ¡sico (modo CPU, RAM limitada)"
        fi
    fi

    echo "âœ¨ ConfiguraciÃ³n recomendada:"
    echo "   ğŸ“¦ Modelo: $MODEL"
    echo "   ğŸ¯ Nivel: $LEVEL"
    echo ""

    # Export for use in other functions
    RECOMMENDED_MODEL="$MODEL"
    RECOMMENDED_LEVEL="$LEVEL"
}

# Function to check existing models
check_existing_models() {
    echo "ğŸ” Verificando modelos existentes..."

    # Use curl to call Ollama API
    MODELS_JSON=$(curl -s "http://${OLLAMA_HOST}/api/tags" 2>/dev/null)

    if [ $? -eq 0 ] && [ -n "$MODELS_JSON" ]; then
        # Parse JSON response to count models
        MODEL_COUNT=$(echo "$MODELS_JSON" | grep -o '"name"' | wc -l)

        if [ "$MODEL_COUNT" -gt 0 ]; then
            echo "âœ… Modelos ya instalados: $MODEL_COUNT"
            echo "   Modelos encontrados:"
            # Extract model names from JSON
            echo "$MODELS_JSON" | grep -o '"name":"[^"]*"' | cut -d'"' -f4 | head -5 | sed 's/^/   ğŸ“¦ /'
            return 0
        else
            echo "âš ï¸  No se encontraron modelos instalados"
            return 1
        fi
    else
        echo "âš ï¸  Error verificando modelos existentes"
        return 1
    fi
}

# Function to install recommended model
install_model() {
    local model=$1

    echo "ğŸ“¥ Instalando modelo: $model"
    echo "â° Esto puede tomar varios minutos dependiendo del modelo y conexiÃ³n..."

    # Check if model is a cloud model - these require signin
    if [[ "$model" == *"-cloud"* ]] || [[ "$model" == *":cloud"* ]]; then
        echo ""
        echo "âš ï¸  Modelo cloud detectado: $model"
        echo "â„¹ï¸  Los modelos cloud requieren autenticaciÃ³n con Ollama"
        echo ""
        echo "ğŸ“‹ IMPORTANTE: Los modelos cloud NO se instalan automÃ¡ticamente"
        echo "   Debes hacer signin manualmente despuÃ©s de la instalaciÃ³n"
        echo ""
        echo "ğŸ’¡ Para instalar modelos cloud despuÃ©s:"
        echo "   1. Accede al contenedor:"
        echo "      docker exec -it msn-ai-ollama /bin/bash"
        echo ""
        echo "   2. Haz signin con Ollama:"
        echo "      ollama signin"
        echo "      (Abre el enlace generado en tu navegador y aprueba el acceso)"
        echo ""
        echo "   3. Instala el modelo:"
        echo "      ollama pull $model"
        echo ""
        echo "   4. Sal del contenedor:"
        echo "      exit"
        echo ""
        return 1
    fi

    # Use curl to pull the model via Ollama API with streaming
    echo "ğŸ”„ Iniciando descarga con progreso..."

    # Start the pull operation with streaming to show progress
    local pull_success=false
    local temp_file="/tmp/ollama_pull_$$.json"

    # Prepare headers
    local headers=(-H "Content-Type: application/json")

    # Add API key header if available
    if [ -n "$OLLAMA_API_KEY" ]; then
        headers+=(-H "Authorization: Bearer ${OLLAMA_API_KEY}")
    fi

    # Make the pull request and capture the response
    if curl -s -X POST "http://${OLLAMA_HOST}/api/pull" \
        "${headers[@]}" \
        -d "{\"name\":\"$model\"}" \
        --max-time 1800 > "$temp_file" 2>/dev/null; then

        echo "âœ… Descarga iniciada para $model"
        pull_success=true
    else
        echo "âŒ Error iniciando la descarga del modelo"
        rm -f "$temp_file"
        return 1
    fi

    if [ "$pull_success" = true ]; then
        # Wait for the model to be available with better error handling
        echo "â³ Verificando instalaciÃ³n del modelo..."
        local attempts=0
        local max_attempts=90  # 15 minutes with 10s intervals

        while [ $attempts -lt $max_attempts ]; do
            local tags_response
            tags_response=$(curl -s --max-time 10 "http://${OLLAMA_HOST}/api/tags" 2>/dev/null)

            if [ $? -eq 0 ] && echo "$tags_response" | grep -q "\"name\":\"$model"; then
                echo "âœ… Modelo $model instalado y disponible"
                rm -f "$temp_file"
                return 0
            fi

            # Show progress every 5 attempts
            if [ $((attempts % 5)) -eq 0 ]; then
                echo "â³ Descargando modelo... ($(( (attempts * 10) / 60 )) min transcurridos)"
            fi

            sleep 10
            attempts=$((attempts + 1))
        done

        echo "âš ï¸  Timeout esperando la instalaciÃ³n del modelo despuÃ©s de 15 minutos"
        echo "   El modelo puede seguir descargÃ¡ndose en segundo plano"
        rm -f "$temp_file"
        return 1
    fi
}

# Function to install required models
install_required_models() {
    echo ""
    echo "ğŸ“¦ Verificando modelos requeridos por defecto..."
    echo "=============================================="

    get_required_models

    # Check if any cloud models were requested
    local has_cloud_models=false
    local cloud_models=("qwen3-vl:235b-cloud" "gpt-oss:120b-cloud" "qwen3-coder:480b-cloud")

    for model in "${cloud_models[@]}"; do
        has_cloud_models=true
        break
    done

    if [ "$has_cloud_models" = true ]; then
        echo ""
        echo "â„¹ï¸  MODELOS CLOUD DISPONIBLES"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""
        echo "Los siguientes modelos cloud estÃ¡n disponibles:"
        for model in "${cloud_models[@]}"; do
            echo "  ğŸ“¦ $model"
        done
        echo ""
        echo "âš ï¸  Los modelos cloud requieren autenticaciÃ³n y NO se instalan"
        echo "   automÃ¡ticamente durante el setup de Docker."
        echo ""
        echo "ğŸ“‹ PARA INSTALAR MODELOS CLOUD:"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""
        echo "1ï¸âƒ£  Accede al contenedor de Ollama:"
        echo "   docker exec -it msn-ai-ollama /bin/bash"
        echo ""
        echo "2ï¸âƒ£  Haz signin con tu cuenta de Ollama:"
        echo "   ollama signin"
        echo ""
        echo "3ï¸âƒ£  Abre el enlace generado en tu navegador"
        echo "   (aparecerÃ¡ algo como: https://ollama.com/connect?name=...)"
        echo ""
        echo "4ï¸âƒ£  Aprueba el acceso en el navegador"
        echo ""
        echo "5ï¸âƒ£  Instala los modelos cloud que necesites:"
        echo "   ollama pull qwen3-vl:235b-cloud"
        echo "   ollama pull gpt-oss:120b-cloud"
        echo "   ollama pull qwen3-coder:480b-cloud"
        echo ""
        echo "6ï¸âƒ£  Verifica que se instalaron:"
        echo "   ollama list"
        echo ""
        echo "7ï¸âƒ£  Sal del contenedor:"
        echo "   exit"
        echo ""
        echo "ğŸ’¡ Los modelos cloud solo funcionan con signin activo"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""
    fi

    local installed_count=0
    local failed_count=0
    local skipped_count=0

    for model in "${REQUIRED_MODELS[@]}"; do
        echo ""
        echo "ğŸ”„ Procesando modelo: $model"

        # Check if model is cloud and we don't have API key
        if [[ "$model" == *"-cloud"* ]] || [[ "$model" == *":cloud"* ]]; then
            if [ -z "$OLLAMA_API_KEY" ]; then
                echo "â­ï¸  Saltando modelo cloud (sin API Key): $model"
                skipped_count=$((skipped_count + 1))
                continue
            fi
        fi

        # Check if model already exists
        if curl -s "http://${OLLAMA_HOST}/api/tags" 2>/dev/null | grep -q "\"name\":\"$model"; then
            echo "âœ… Modelo $model ya estÃ¡ instalado"
            installed_count=$((installed_count + 1))
        else
            echo "ğŸ“¥ Instalando modelo: $model"

            if install_model "$model"; then
                echo "âœ… Modelo $model instalado exitosamente"
                installed_count=$((installed_count + 1))
            else
                echo "âš ï¸  Error instalando modelo $model"
                failed_count=$((failed_count + 1))
            fi
        fi
    done

    echo ""
    echo "ğŸ“Š Resumen de instalaciÃ³n de modelos requeridos:"
    echo "   âœ… Instalados correctamente: $installed_count/${#REQUIRED_MODELS[@]}"
    echo "   âš ï¸  Fallidos: $failed_count/${#REQUIRED_MODELS[@]}"
    if [ $skipped_count -gt 0 ]; then
        echo "   â­ï¸  Saltados (sin API Key): $skipped_count/${#REQUIRED_MODELS[@]}"
    fi
    echo ""

    # Always return success - MSN-AI can work without pre-installed models
    if [ $installed_count -gt 0 ]; then
        echo "âœ… Setup completado con modelos instalados"
        return 0
    elif [ $skipped_count -gt 0 ]; then
        echo "â„¹ï¸  Modelos cloud saltados por falta de API Key (normal)"
        echo "   MSN-AI funcionarÃ¡ y podrÃ¡s instalar modelos manualmente"
        return 0
    elif [ $failed_count -gt 0 ]; then
        echo "âš ï¸  Algunos modelos fallaron, pero MSN-AI puede continuar"
        echo "   Los modelos pueden instalarse manualmente desde la interfaz"
        return 0
    else
        echo "âš ï¸  No se instalaron modelos automÃ¡ticamente"
        echo "   Esto no afecta el funcionamiento de MSN-AI"
        return 0
    fi
}

# Function to test model
test_model() {
    local model=$1

    echo "ğŸ§ª Probando modelo: $model"

    # Test the model with a simple prompt
    TEST_RESPONSE=$(curl -s -X POST "http://${OLLAMA_HOST}/api/generate" \
        -H "Content-Type: application/json" \
        -d "{\"model\":\"$model\",\"prompt\":\"Hola, Â¿funcionas correctamente?\",\"stream\":false}" 2>/dev/null)

    if [ $? -eq 0 ] && echo "$TEST_RESPONSE" | grep -q '"response"'; then
        echo "âœ… Modelo $model funciona correctamente"

        # Extract response for display
        RESPONSE_TEXT=$(echo "$TEST_RESPONSE" | grep -o '"response":"[^"]*"' | cut -d'"' -f4 | head -c 100)
        echo "ğŸ¤– Respuesta de prueba: $RESPONSE_TEXT..."

        return 0
    else
        echo "âš ï¸  Error probando el modelo $model"
        return 1
    fi
}

# Function to save configuration
save_config() {
    local model=$1
    local config_file="/app/data/config/ai-config.json"

    echo "ğŸ’¾ Guardando configuraciÃ³n..."

    mkdir -p "$(dirname "$config_file")"

    cat > "$config_file" << EOF
{
  "version": "1.0.0",
  "setup_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "setup_mode": "docker",
  "hardware": {
    "cpu_cores": $CPU_CORES,
    "ram_gb": $RAM_GB,
    "gpu_info": "$GPU_INFO",
    "vram_gb": $VRAM_GB
  },
  "ai_config": {
    "recommended_model": "$model",
    "level": "$RECOMMENDED_LEVEL",
    "ollama_host": "$OLLAMA_HOST"
  },
  "container_info": {
    "hostname": "$(hostname)",
    "user": "$(whoami)"
  }
}
EOF

    echo "âœ… ConfiguraciÃ³n guardada en: $config_file"
}

# Main setup function
main() {
    echo "ğŸš€ Iniciando configuraciÃ³n de IA para MSN-AI..."
    echo "â° Timeout configurado: ${SETUP_TIMEOUT}s"

    # Set timeout for entire process
    local start_time=$(date +%s)

    # Wait for Ollama service with retries
    echo "ğŸ“¡ Estableciendo conexiÃ³n con Ollama..."
    local ollama_attempts=0
    while [ $ollama_attempts -lt 3 ]; do
        if wait_for_ollama; then
            break
        fi

        ollama_attempts=$((ollama_attempts + 1))
        if [ $ollama_attempts -lt 3 ]; then
            echo "ğŸ”„ Reintentando conexiÃ³n con Ollama (intento $((ollama_attempts + 1))/3)..."
            sleep 15
        fi
    done

    if [ $ollama_attempts -eq 3 ]; then
        echo "âŒ No se pudo establecer conexiÃ³n con Ollama despuÃ©s de 3 intentos"
        echo "   Posibles causas:"
        echo "   - Ollama container aÃºn iniciÃ¡ndose"
        echo "   - Recursos del sistema insuficientes"
        echo "   - Problemas de red entre contenedores"
        echo ""
        echo "ğŸ’¡ El contenedor MSN-AI funcionarÃ¡ sin IA hasta que Ollama estÃ© disponible"
        echo "âš ï¸  Setup de IA cancelado - MSN-AI continuarÃ¡ sin modelos preinstalados"
        exit 0
    fi

    # Check timeout
    local current_time=$(date +%s)
    local elapsed=$((current_time - start_time))
    if [ $elapsed -ge $SETUP_TIMEOUT ]; then
        echo "â° Timeout alcanzado durante la conexiÃ³n inicial"
        echo "âš ï¸  Setup de IA cancelado por timeout - MSN-AI continuarÃ¡ sin modelos preinstalados"
        exit 0
    fi

    # Detect hardware
    detect_hardware

    # Recommend optimal model
    recommend_model

    # Install required default models
    echo "ğŸ“¦ Instalando modelos requeridos por defecto..."
    if ! install_required_models; then
        echo "âš ï¸  Error instalando modelos requeridos, pero continuando setup..."
    fi

    # Check timeout after installing required models
    current_time=$(date +%s)
    elapsed=$((current_time - start_time))

    if [ $elapsed -lt $((SETUP_TIMEOUT - 60)) ]; then
        # Check if models already exist beyond required ones
        if check_existing_models; then
            echo "â„¹ï¸  Modelos adicionales encontrados"
            echo "   ConfiguraciÃ³n automÃ¡tica: Usando modelos existentes"
            echo "   Modelo recomendado para este hardware: $RECOMMENDED_MODEL"
        else
            echo "ğŸ“¦ Instalando modelo recomendado adicional: $RECOMMENDED_MODEL"

            # Check timeout before starting model installation
            current_time=$(date +%s)
            elapsed=$((current_time - start_time))
            if [ $elapsed -ge $((SETUP_TIMEOUT - 60)) ]; then
                echo "â° Tiempo insuficiente para instalar modelo adicional"
            else
                if install_model "$RECOMMENDED_MODEL"; then
                    echo "âœ… InstalaciÃ³n exitosa del modelo recomendado"

                    # Test the model if we have time
                    current_time=$(date +%s)
                    elapsed=$((current_time - start_time))
                    if [ $elapsed -lt $((SETUP_TIMEOUT - 30)) ]; then
                        if test_model "$RECOMMENDED_MODEL"; then
                            echo "ğŸ‰ ConfiguraciÃ³n de IA completada exitosamente"
                        else
                            echo "âš ï¸  Modelo instalado pero fallÃ³ la prueba (puede ser normal al inicio)"
                        fi
                    else
                        echo "â° Saltando test del modelo por tiempo"
                    fi
                else
                    echo "âš ï¸  Error en la instalaciÃ³n del modelo recomendado"
                    echo "   Continuando con los modelos requeridos ya instalados"
                fi
            fi
        fi
    else
        echo "â° Tiempo insuficiente para modelos adicionales"
        echo "   Los modelos requeridos ya fueron instalados"
    fi

    # Save configuration (allow to fail gracefully)
    if ! save_config "$RECOMMENDED_MODEL"; then
        echo "âš ï¸  No se pudo guardar la configuraciÃ³n, pero continuando..."
    fi

    echo ""
    echo "ğŸ‰ ConfiguraciÃ³n de MSN-AI Docker completada"
    echo "============================================="
    echo ""
    echo "ğŸ“Š RESUMEN DE MODELOS:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

    # Show installed local models
    echo ""
    echo "âœ… Modelos locales instalados:"
    if curl -s "http://${OLLAMA_HOST}/api/tags" 2>/dev/null | grep -q "\"name\""; then
        curl -s "http://${OLLAMA_HOST}/api/tags" 2>/dev/null | grep -o '"name":"[^"]*"' | cut -d'"' -f4 | while read model; do
            if [[ ! "$model" == *"-cloud"* ]]; then
                echo "   ğŸ“¦ $model"
            fi
        done
    else
        echo "   (ninguno aÃºn)"
    fi

    # Show cloud models information
    echo ""
    echo "â˜ï¸  Modelos cloud (requieren signin):"
    local cloud_models=("qwen3-vl:235b-cloud" "gpt-oss:120b-cloud" "qwen3-coder:480b-cloud")
    for model in "${cloud_models[@]}"; do
        if curl -s "http://${OLLAMA_HOST}/api/tags" 2>/dev/null | grep -q "\"name\":\"$model"; then
            echo "   âœ… $model (instalado)"
        else
            echo "   â­ï¸  $model (pendiente - requiere signin manual)"
        fi
    done

    echo ""
    echo "ğŸ¤– Modelo adicional recomendado: $RECOMMENDED_MODEL"
    echo "ğŸ¯ Nivel: $RECOMMENDED_LEVEL"
    echo "âš–ï¸ Licencia: GPL-3.0"
    echo "ğŸ³ Host Ollama: $OLLAMA_HOST"
    echo "ğŸ’¾ ConfiguraciÃ³n: /app/data/config/ai-config.json"
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ’¡ PARA USAR MODELOS CLOUD:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "   docker exec -it msn-ai-ollama ollama signin"
    echo "   (Abre el enlace en tu navegador y aprueba)"
    echo "   docker exec -it msn-ai-ollama ollama pull qwen3-vl:235b-cloud"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "â±ï¸  Tiempo total: $(($(date +%s) - start_time))s"
    echo ""
    echo "âœ… MSN-AI estÃ¡ listo para usar (setup completado)"
    echo "   Los modelos pueden continuar descargÃ¡ndose en segundo plano"
    echo "============================================="
}

# Run main function with error handling
if main "$@"; then
    echo "âœ… Setup finalizado exitosamente"
    exit 0
else
    echo "âš ï¸  Setup completado con advertencias, pero MSN-AI puede continuar"
    exit 0
fi
